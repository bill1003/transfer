{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas #ipython notebook\n",
    "titanic = pandas.read_csv(\"titanic_train.csv\")\n",
    "titanic.head(5)\n",
    "#print (titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.361582    0.523008   \n",
      "std     257.353842    0.486592    0.836071   13.019697    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "print(titanic[\"Embarked\"].unique())\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('S')\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 8.99877810e-02,  9.60756206e-01,  5.92676278e-01,  9.31138728e-01,\n",
       "         5.29343071e-02,  1.70275685e-01,  3.69943590e-01,  1.03474847e-01,\n",
       "         5.21597906e-01,  8.74491050e-01,  6.48883611e-01,  8.29742769e-01,\n",
       "         1.34797198e-01, -1.61126844e-01,  6.58141307e-01,  6.39819748e-01,\n",
       "         1.51733875e-01,  2.95432718e-01,  5.35377959e-01,  6.21007683e-01,\n",
       "         2.61872592e-01,  2.62687561e-01,  7.31739160e-01,  5.05995897e-01,\n",
       "         5.61398567e-01,  3.35039734e-01,  1.30338808e-01,  4.68765767e-01,\n",
       "         6.60737753e-01,  9.10819218e-02,  4.77223920e-01,  1.04220026e+00,\n",
       "         6.60691613e-01,  8.71539273e-02,  5.28550732e-01,  4.01874338e-01,\n",
       "         1.30340307e-01,  1.29339672e-01,  5.72717129e-01,  6.65238822e-01,\n",
       "         4.83215779e-01,  7.60807408e-01,  1.30578363e-01,  8.71867121e-01,\n",
       "         7.09855487e-01,  9.11369897e-02,  1.39181745e-01,  6.60691613e-01,\n",
       "         6.82833485e-02,  6.06254374e-01,  4.92254383e-02,  1.29250392e-01,\n",
       "         9.02668258e-01,  7.51677954e-01,  3.19636822e-01,  5.05995897e-01,\n",
       "         8.23411477e-01,  1.27611544e-01,  8.16516947e-01, -3.70209060e-02,\n",
       "         1.63085464e-01,  9.57981340e-01,  3.96742103e-01,  6.16138409e-02,\n",
       "         5.42714233e-01,  6.62112275e-02,  7.79751268e-01,  1.40293401e-01,\n",
       "         4.40592742e-01,  3.50534388e-02,  2.72709814e-01,  4.26360339e-01,\n",
       "         3.55241143e-01,  1.10226880e-01,  8.66078358e-02,  1.07366720e-01,\n",
       "         9.10819218e-02,  9.11369897e-02,  3.82661024e-01,  5.72471068e-01,\n",
       "         1.24221410e-01,  8.61972872e-02,  6.60705005e-01,  5.10138486e-01,\n",
       "         8.45241581e-01,  4.56477760e-01,  3.22699204e-02,  9.11369897e-02,\n",
       "         9.37604538e-01,  1.12967094e-01,  8.56794636e-02,  1.34727274e-01,\n",
       "         3.83320807e-01,  6.14970393e-03, -7.83320148e-02,  9.11369897e-02,\n",
       "         3.10516665e-01,  5.49345421e-01,  7.23544338e-01,  2.33721448e-01,\n",
       "         5.81750798e-01,  9.10819218e-02,  5.25738424e-01,  6.40651310e-02,\n",
       "        -2.52427240e-02,  9.10819218e-02,  6.19865700e-01,  9.10387818e-02,\n",
       "         3.65066610e-02,  6.32939707e-01,  4.08195377e-01,  6.63657306e-01,\n",
       "         1.23882146e-01,  5.92491292e-01,  6.83623624e-01,  1.29295032e-01,\n",
       "        -6.19221217e-02,  2.59223480e-01,  6.09655955e-01,  5.30794378e-01,\n",
       "         2.88023805e-01,  9.11369897e-02,  2.82857942e-01,  7.61542726e-01,\n",
       "         3.45640063e-01,  1.85484998e-01,  1.70022737e-01,  1.12642722e-01,\n",
       "         5.59420117e-01, -2.02485747e-03,  1.03290733e-01,  1.34440079e-01,\n",
       "         4.46807623e-01,  7.51677954e-01,  3.11805296e-01,  3.62947385e-01,\n",
       "         9.75724449e-01,  4.29554800e-01,  1.57043954e-01,  5.82928575e-01,\n",
       "         5.57105476e-01,  6.14443886e-01,  5.72812834e-01,  2.18783352e-01,\n",
       "         3.49472299e-01,  2.86040080e-01,  9.65037360e-02,  5.60916106e-01,\n",
       "         1.86919710e-01,  2.19027353e-01,  1.69739986e-01,  1.00690768e+00,\n",
       "        -5.89449777e-02, -4.15452572e-02,  9.08736139e-02,  3.95827915e-01,\n",
       "         7.26175962e-01,  8.02219375e-02,  9.13557255e-02, -2.22536096e-01,\n",
       "        -2.66919104e-02,  7.21593360e-01,  1.01953834e-01,  1.51388512e-01,\n",
       "         8.19705948e-02,  1.32518461e-01,  9.70245311e-01,  3.28974893e-01,\n",
       "         5.02576476e-01,  1.08437940e-01,  3.25183297e-01,  1.40818823e-01,\n",
       "         6.63268211e-01,  1.29295032e-01,  3.90965934e-01,  7.86503606e-02,\n",
       "        -3.68524682e-02,  9.13671691e-01,  2.84517666e-01,  4.46019673e-02,\n",
       "         2.68132779e-01,  3.35661255e-01,  1.96299597e-03,  3.51470400e-01,\n",
       "         6.51010647e-01,  5.11174133e-01,  6.29850621e-01,  4.10021732e-01,\n",
       "         4.03081359e-02,  4.74217131e-02,  7.64271489e-01,  3.44550453e-01,\n",
       "         5.97245007e-01,  3.69521460e-01,  9.46062691e-01,  9.12083149e-01,\n",
       "         1.70022737e-01, -1.85251802e-02,  6.60691613e-01,  8.07931698e-01,\n",
       "         9.16548133e-02, -2.22536096e-01,  5.78367977e-02,  3.48321010e-02,\n",
       "         1.45712251e-01,  6.91179799e-01,  3.84837497e-02,  1.45383056e-01,\n",
       "         7.26181926e-01,  4.78394987e-01,  1.12609974e-01,  7.50755869e-01,\n",
       "         1.23596450e-01,  2.84517666e-01,  1.36414068e-01,  1.01395495e+00,\n",
       "         5.87218752e-01,  1.90418359e-01,  1.02889863e+00,  2.83624866e-01,\n",
       "         1.56627303e-01,  3.00890244e-01, -3.43861103e-02,  9.10819218e-02,\n",
       "         4.37274991e-01,  1.24346402e-01,  3.43657653e-01,  1.31782740e-01,\n",
       "         3.50007979e-01,  4.53816408e-01,  9.41986239e-01,  8.55812557e-02,\n",
       "         1.26427969e-01,  5.14461976e-01,  3.16370023e-01,  5.81627306e-01,\n",
       "         1.79146187e-01,  8.33217359e-01,  3.43657653e-01,  2.67886176e-01,\n",
       "         5.89980704e-01,  6.29850621e-01,  2.89082393e-01,  1.23551810e-01,\n",
       "         1.19423755e-01,  4.49914049e-01,  5.98080236e-01,  7.41700785e-01,\n",
       "         3.95976588e-01,  1.24570927e-01,  9.08512939e-02,  5.10217925e-01,\n",
       "         3.17243789e-01,  4.94880818e-02,  4.48434902e-01,  5.51647950e-01,\n",
       "         1.05176735e+00,  1.00396283e+00,  1.16824364e+00,  6.37295280e-01,\n",
       "         1.70022737e-01,  3.47081525e-02,  3.23790141e-01,  4.27827834e-01,\n",
       "         6.60691613e-01,  2.50879710e-01,  1.07703504e-04,  7.38026906e-02,\n",
       "         8.41682429e-01,  9.94221666e-01,  5.04388858e-01,  1.04634754e-01,\n",
       "         6.84091736e-01,  4.60920013e-01,  6.60691613e-01,  7.87205387e-01,\n",
       "         4.88920786e-01,  2.90790162e-01,  1.24446245e-01,  4.80968077e-01,\n",
       "        -3.19057282e-02,  9.10670657e-02,  1.57145126e-01,  1.40254724e-01,\n",
       "         5.02603260e-01,  1.03564537e-01,  8.07397611e-02,  1.23827078e-01,\n",
       "         2.19027353e-01,  6.93436769e-01,  1.02306096e+00,  1.07151871e+00,\n",
       "         2.91224311e-01,  6.03921666e-01,  1.12912026e-01,  5.42714233e-01,\n",
       "         1.54899175e-01]),\n",
       " array([ 1.13774791,  0.44173212,  0.98551347,  0.66915371,  0.08254228,\n",
       "         0.15142624,  0.83642014,  0.09704526,  0.64711481,  1.03845173,\n",
       "         1.06064212,  0.24647842,  0.98364902,  1.04411609,  1.10195734,\n",
       "         0.72596387,  0.09692709,  0.11388411,  0.60824987,  0.74905725,\n",
       "         0.090424  ,  1.00314273,  0.91588368,  0.13679886,  0.10365487,\n",
       "         0.82296458,  0.755174  , -0.27746285,  1.0035964 , -0.12636043,\n",
       "         0.70865678,  0.52438799,  1.06900476,  0.58044138,  0.32246331,\n",
       "         0.45904751,  0.0848131 ,  0.96838383,  0.09692709,  0.4123739 ,\n",
       "         0.96908901, -0.01732698,  0.33119158,  0.38953146,  0.97455471,\n",
       "         0.26457991,  0.28476325,  0.21075768,  0.78939013,  0.68174567,\n",
       "         0.5508181 ,  0.21132238,  0.00332574,  0.1315846 ,  0.44518065,\n",
       "         0.16116388,  0.07440511,  0.13363265,  0.09815645,  0.98913539,\n",
       "         0.69520122,  0.66925272,  0.66925272, -0.05732283,  0.25605759,\n",
       "         0.51306171,  0.04918447,  0.12689844,  0.08297663,  0.74556032,\n",
       "         0.63153497,  0.66915371,  1.03349593,  0.46795359,  0.11283671,\n",
       "         0.15759527,  0.5998862 ,  0.6125967 ,  0.96615292,  0.63469796,\n",
       "         0.6051113 ,  0.18499302,  0.15738453,  1.03364995,  0.80043282,\n",
       "         0.07003835,  0.85871777,  0.09692709,  0.37822123,  0.03771546,\n",
       "         0.70865678,  0.17123866,  0.87293786,  0.38692632,  0.14394491,\n",
       "        -0.00364112,  1.02362819,  0.60920867,  0.13721713,  0.57461098,\n",
       "         0.1534423 ,  0.29630296,  0.76221079,  0.0229439 ,  0.11050082,\n",
       "         0.59310377,  0.05272741,  0.64923598,  0.18004866, -0.05792355,\n",
       "         0.37724772,  0.14392897,  0.44776777,  0.09692709,  0.17057126,\n",
       "         0.97573347,  0.2546175 , -0.01069499,  0.59494436,  0.67712284,\n",
       "         0.81048116,  0.25112435,  0.7091068 ,  0.13414671,  0.21833626,\n",
       "         0.09018337,  0.5398775 ,  0.11371054,  0.09643219,  0.72214613,\n",
       "         0.83299143,  0.1712546 ,  0.07013414,  0.43870508,  0.5508181 ,\n",
       "         0.62795723,  0.17034196,  0.26289071,  1.03283656,  0.54234647,\n",
       "         0.66429253,  0.2888594 ,  0.24248073,  0.59832765,  0.15197868,\n",
       "         0.06672256,  0.76247901,  0.09709316,  0.62328105,  0.85873908,\n",
       "         0.39833841,  0.68526385,  0.28026543,  0.15249025,  0.0558822 ,\n",
       "         0.46338875,  0.3322838 ,  0.09704526,  0.12741893,  0.18977726,\n",
       "         0.90570685,  0.61255203,  0.1712546 ,  0.3041495 ,  0.05667859,\n",
       "         0.32003504,  0.13002433,  0.09704526,  0.02900113,  0.2546175 ,\n",
       "         0.25032727,  0.17123545,  0.71385691,  0.09643219,  0.03023685,\n",
       "         0.67057269,  0.83394424,  0.63668087,  0.45820842,  0.18004866,\n",
       "         0.03925263,  0.13700639,  0.76347615, -0.01610677,  0.2546175 ,\n",
       "        -0.05096587,  0.36065035,  0.49526401,  0.44776777,  0.88783867,\n",
       "         0.27650531,  0.0835897 ,  0.17095571,  0.0558822 ,  0.14352664,\n",
       "         0.26008209,  0.20422092,  0.14413971,  0.13917582,  0.78823881,\n",
       "         0.10244795,  0.983009  ,  0.12376157,  0.17152021,  0.71624816,\n",
       "         0.66906113,  0.5355726 ,  1.06327957,  0.55601524,  0.71952689,\n",
       "         0.43870508,  0.10813802,  0.14762674,  0.16452683,  0.09704526,\n",
       "         0.38468169,  0.77378051,  0.12353167,  0.31660245,  0.72019649,\n",
       "         0.18382257,  0.6683239 ,  0.07001598,  0.97445504,  0.13729376,\n",
       "         0.13363265,  0.88062695,  0.13363587,  0.08715737,  0.61255203,\n",
       "         0.5883169 ,  0.0229439 ,  0.18684089,  0.88743056,  0.13363587,\n",
       "         0.14770832,  0.62385335,  0.58195819,  0.89464072,  0.32433284,\n",
       "         1.0215796 ,  0.10198815,  1.01250232,  0.89757009,  0.52011358,\n",
       "         0.50665802,  0.19733591,  0.33882963,  0.19608356,  0.78269614,\n",
       "         0.3024605 ,  0.01303333,  0.35740293,  0.59528255,  0.2812701 ,\n",
       "         0.1713153 ,  0.17399933,  0.63510029,  0.2099606 ,  0.79897366,\n",
       "         0.62993975,  0.84335812,  0.49799211,  0.1712546 ,  0.01619374,\n",
       "         0.26496308,  0.09704526,  0.59494436,  0.03570385,  0.1574771 ,\n",
       "         0.55964686,  0.13363587,  0.0699841 ,  0.03391958,  0.68692335,\n",
       "         0.38475832,  0.66915371,  0.17777861,  0.16253816,  0.72211234,\n",
       "         0.83479538,  0.58677963,  0.07003835,  0.735757  ,  0.90451305,\n",
       "         0.09962007,  0.43250553,  0.13477258,  1.02529894,  0.13828479,\n",
       "         0.24105043,  0.13741193,  0.09704526,  0.04924194,  0.80169436,\n",
       "        -0.03139561,  0.64987806]),\n",
       " array([ 1.72889219e-01,  1.70294715e-02,  7.82616935e-01, -8.34788848e-03,\n",
       "         1.47022266e-01,  3.10888595e-01,  7.28261340e-01,  1.01479914e-01,\n",
       "         4.24565622e-01,  1.57316587e-02,  4.37708069e-01,  1.44204264e-02,\n",
       "         9.07678482e-02,  4.33913871e-01,  8.26537251e-01,  8.45262338e-01,\n",
       "         5.42776171e-01,  1.01763663e-01,  6.70148479e-01,  1.92163452e-01,\n",
       "         6.39359534e-02,  7.62650655e-01,  3.10124701e-02,  5.90024631e-01,\n",
       "         8.31356231e-01,  2.78648916e-01,  1.08309653e-01,  3.04531238e-01,\n",
       "         1.50864127e-01,  1.38986099e-01,  1.36219795e-01,  2.51197915e-01,\n",
       "         2.02625887e-01,  9.72357134e-01,  1.12191979e-01,  1.92169054e-01,\n",
       "         1.50211875e-01, -2.14264992e-02,  4.52451020e-01,  4.38789988e-01,\n",
       "         6.04820088e-01,  7.89326541e-01,  8.00459867e-02,  2.10435721e-01,\n",
       "         5.70885269e-01,  5.70841743e-02,  1.44342132e-01,  1.00451104e+00,\n",
       "         6.42312317e-01,  8.51755703e-02,  7.33373007e-01,  3.09602117e-01,\n",
       "         1.49684208e-01,  3.22228832e-01,  1.01595923e-01,  6.50604478e-01,\n",
       "         1.01479914e-01,  8.45026241e-01,  1.38791822e-01,  7.14365273e-01,\n",
       "         7.68287651e-01,  1.84938938e-01,  1.01479914e-01,  6.54218524e-01,\n",
       "         2.93878313e-01,  2.96413137e-01,  1.92833539e-01,  8.27498735e-02,\n",
       "         3.28441263e-01,  5.87658439e-02,  1.02674988e-01,  1.42090676e-01,\n",
       "         2.83166248e-01,  1.01520440e-01,  2.10876914e-02,  9.01930011e-01,\n",
       "         6.80182444e-01,  3.63633521e-01,  4.29834748e-02,  2.51030051e-01,\n",
       "         2.71459394e-01,  1.55080767e-01,  1.20174297e-01,  6.76615822e-01,\n",
       "         5.21604336e-01,  2.74876851e-01,  7.14261845e-01,  4.63722197e-01,\n",
       "         1.43882255e-01, -3.38493769e-02,  5.08333972e-02,  2.88240761e-01,\n",
       "         4.71949096e-03,  1.48920991e-01,  1.55073789e-01,  9.65241409e-01,\n",
       "         3.61956120e-01,  8.01212426e-01,  8.51755703e-02,  1.63090365e-01,\n",
       "         2.58489938e-01,  1.38385623e-01,  1.57316587e-02,  7.14397446e-01,\n",
       "         2.98282232e-01,  2.65779163e-02,  9.41922468e-01,  3.92478820e-01,\n",
       "         7.25879907e-01,  2.08234335e-01,  7.05625434e-02,  2.03820545e-01,\n",
       "         6.98106244e-01,  3.54986591e-01,  9.42312534e-01,  1.08182230e-01,\n",
       "         1.01115214e+00,  4.29882986e-01,  2.72580965e-01,  9.55913060e-02,\n",
       "         1.38553363e-01,  1.49766670e-01,  8.76445205e-01,  7.95521275e-01,\n",
       "         1.89563479e-01,  7.47402760e-02,  9.05943831e-01,  1.19035222e-01,\n",
       "         2.34961953e-01,  1.49265429e-01,  3.84688624e-01,  1.44070963e-01,\n",
       "         6.51000458e-01,  7.14396037e-01,  2.37161612e-01,  5.98123216e-01,\n",
       "         8.84762775e-01,  2.34195832e-01,  2.71459394e-01,  2.93878313e-01,\n",
       "         2.93878313e-01,  9.60495497e-02,  4.82543535e-01,  2.74738708e-01,\n",
       "         1.01479914e-01,  1.01479914e-01,  4.28725578e-01,  3.27845711e-01,\n",
       "         8.83507841e-01,  7.85083053e-02,  8.54020195e-02,  1.53868294e-01,\n",
       "         1.25458500e-01,  7.78614476e-01,  4.27536886e-01,  1.76095354e-01,\n",
       "         8.78367308e-01,  2.23270579e-01,  7.41615725e-02,  1.28260077e-01,\n",
       "         6.34105869e-01,  3.76826088e-01,  1.01513462e-01,  3.21161697e-01,\n",
       "         6.92919862e-02,  9.05219168e-01,  9.92643346e-02,  3.21100762e-02,\n",
       "         1.89869119e-01,  8.47257439e-01,  1.65792833e-01,  7.70032759e-01,\n",
       "         4.70822280e-01,  7.01001762e-01,  1.45018183e-01,  7.98992141e-02,\n",
       "         1.22365867e-01, -5.62678525e-03,  6.34840292e-01,  1.47022266e-01,\n",
       "         6.21554022e-01,  1.55089154e-01,  1.92163452e-01,  7.45360827e-01,\n",
       "         1.92167645e-01,  8.15272492e-01,  7.49589740e-01,  9.59168970e-01,\n",
       "         4.23369546e-01,  6.56067455e-02,  1.17831761e-01,  1.17764665e-01,\n",
       "         6.77402825e-01,  1.31033823e-01,  2.11184136e-01,  3.61128670e-01,\n",
       "         1.92163452e-01,  3.27009298e-01,  2.80865752e-01,  4.73809464e-01,\n",
       "         1.17548012e-01,  2.08181789e-01,  8.39842956e-01,  6.07376016e-01,\n",
       "         1.36308792e-01,  5.71394060e-01,  2.34961953e-01,  7.32664113e-01,\n",
       "         4.58929866e-01,  2.99802486e-01,  1.07144857e-01,  8.54523415e-02,\n",
       "         3.79873628e-01,  6.77309159e-01,  2.08181789e-01,  8.74780819e-01,\n",
       "         1.12194764e-01,  3.71105893e-02,  2.30444621e-01,  5.78112549e-01,\n",
       "         8.80381008e-02,  4.38789988e-01,  6.50478673e-01,  2.52145211e-01,\n",
       "         2.16244600e-02,  7.72356638e-02,  7.64956968e-01,  1.06578734e-01,\n",
       "         3.85229660e-01,  6.33022282e-01,  6.89918839e-02,  1.92431836e-01,\n",
       "         8.51755703e-02,  4.59963761e-01,  1.92163452e-01,  7.52074841e-01,\n",
       "         6.94810438e-01,  3.74543331e-01,  1.47020857e-01,  1.28274033e-01,\n",
       "         1.54904640e-01,  8.83372143e-01,  1.38714930e-01,  1.01428183e-01,\n",
       "         6.37514393e-02,  4.74143535e-01,  1.44318380e-01,  3.32209243e-01,\n",
       "         9.85223737e-01,  1.12472244e-01,  1.60139061e-01,  2.66114644e-02,\n",
       "        -2.41362640e-01,  1.09304997e-01,  2.65882719e-01,  9.34799595e-01,\n",
       "         6.65962224e-02, -1.44857067e-01,  7.32175244e-01,  1.01756702e+00,\n",
       "         6.57625381e-01,  6.82274953e-01,  7.78507074e-01,  3.06694232e-01,\n",
       "         7.03120381e-01,  1.47020857e-01, -5.35194672e-02,  2.63450207e-01,\n",
       "         8.45198988e-01,  2.80865752e-01,  2.88522280e-01,  7.14342083e-01,\n",
       "         7.98068552e-01,  4.05781543e-01,  1.00941736e-01,  1.92789366e-01,\n",
       "         1.12191979e-01,  8.05473642e-01,  4.10332423e-01, -6.55145848e-04,\n",
       "         7.89310178e-01,  7.38879084e-01,  1.43673989e-01,  1.49684208e-01,\n",
       "         1.01479914e-01,  8.33962978e-01,  8.06527571e-01,  7.46997500e-02,\n",
       "         6.54965242e-01,  2.67936850e-01,  1.17831761e-01,  6.75775470e-01,\n",
       "         2.72454182e-01,  9.99158265e-01,  5.87835137e-01,  4.84754956e-01,\n",
       "         1.70739321e-01])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2615039281705948\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7878787878787877\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_test = pandas.read_csv(\"test.csv\")\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7856341189674523\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "kf = cross_validation.KFold(titanic.shape[0], 3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Major         2\n",
      "Mlle          2\n",
      "Col           2\n",
      "Capt          1\n",
      "Countess      1\n",
      "Mme           1\n",
      "Ms            1\n",
      "Jonkheer      1\n",
      "Lady          1\n",
      "Sir           1\n",
      "Don           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GuoYu/miniconda3/envs/env/lib/python3.6/site-packages/sklearn/utils/__init__.py:127: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"NameLength\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27946127946127947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\",]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pandas.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11681943, 0.47836987, 0.12614048, 0.13097708, 0.52107272,\n",
       "       0.14351509, 0.64086375, 0.18002627, 0.67802231, 0.12110664,\n",
       "       0.12104744, 0.20901962, 0.91068965, 0.10891017, 0.89143306,\n",
       "       0.87714269, 0.16348534, 0.13906931, 0.54105377, 0.55662058,\n",
       "       0.22420124, 0.53718352, 0.90572929, 0.38889821, 0.88385196,\n",
       "       0.10357086, 0.90910138, 0.13745621, 0.31046113, 0.1266512 ,\n",
       "       0.11663426, 0.18274307, 0.55222145, 0.49649779, 0.42414476,\n",
       "       0.14190148, 0.5097587 , 0.52454348, 0.13270036, 0.28366139,\n",
       "       0.11144781, 0.46618779, 0.0999618 , 0.83421925, 0.89960308,\n",
       "       0.14982951, 0.31592763, 0.13788495, 0.89105091, 0.54190897,\n",
       "       0.35666131, 0.17717223, 0.83074005, 0.87996462, 0.17558166,\n",
       "       0.13738875, 0.10666907, 0.12343384, 0.12099323, 0.9128551 ,\n",
       "       0.13098613, 0.15341529, 0.12993431, 0.66574184, 0.66341763,\n",
       "       0.87273821, 0.67239645, 0.28826312, 0.35235967, 0.85566511,\n",
       "       0.66225137, 0.12701486, 0.55392305, 0.36739876, 0.91110735,\n",
       "       0.41201027, 0.13013567, 0.83673046, 0.15613989, 0.66225137,\n",
       "       0.68126047, 0.20605054, 0.20382454, 0.12104744, 0.18485201,\n",
       "       0.13129541, 0.65681561, 0.53031943, 0.65490661, 0.7987881 ,\n",
       "       0.53765995, 0.12103592, 0.89138279, 0.13013567, 0.28405691,\n",
       "       0.12344901, 0.86794385, 0.14665909, 0.58601586, 0.12260391,\n",
       "       0.90434217, 0.14730313, 0.13788495, 0.12261977, 0.62258308,\n",
       "       0.13155404, 0.14606445, 0.13788495, 0.13019897, 0.1747259 ,\n",
       "       0.14285637, 0.65491346, 0.89529098, 0.67147699, 0.88346925,\n",
       "       0.13991227, 0.11804329, 0.69614332, 0.36668206, 0.86243053,\n",
       "       0.87650636, 0.12608344, 0.90276979, 0.12098591, 0.13788495,\n",
       "       0.56973992, 0.12607685, 0.6373454 , 0.13339624, 0.13340097,\n",
       "       0.12723238, 0.516065  , 0.23922865, 0.10791048, 0.09896431,\n",
       "       0.12430648, 0.13345732, 0.16213663, 0.52031607, 0.12232514,\n",
       "       0.20713034, 0.90530415, 0.19746624, 0.16153256, 0.42927458,\n",
       "       0.10486884, 0.33642421, 0.13517918, 0.46618779, 0.34475031,\n",
       "       0.91431763, 0.13214259, 0.106908  , 0.48984982, 0.11273495,\n",
       "       0.12427392, 0.91070653, 0.57993806, 0.42927458, 0.51275443,\n",
       "       0.65490269, 0.5788139 , 0.82115224, 0.12096213, 0.2897616 ,\n",
       "       0.58588482, 0.30129764, 0.14606414, 0.90250897, 0.52259532,\n",
       "       0.12101447, 0.13298743, 0.12418074, 0.13206749, 0.13196412,\n",
       "       0.8729528 , 0.8763491 , 0.2966958 , 0.83391074, 0.85559817,\n",
       "       0.15613989, 0.33351255, 0.90219659, 0.13788495, 0.91719144,\n",
       "       0.13602615, 0.85484209, 0.12240938, 0.14217439, 0.13560305,\n",
       "       0.13487572, 0.25547156, 0.4994872 , 0.12728693, 0.71978347,\n",
       "       0.10795079, 0.855151  , 0.58992535, 0.16645233, 0.53981907,\n",
       "       0.64868974, 0.66326561, 0.60979494, 0.87334866, 0.16322206,\n",
       "       0.25696069, 0.63084589, 0.16482157, 0.88985628, 0.12345941,\n",
       "       0.12849223, 0.12096689, 0.24674446, 0.80201864, 0.41248946,\n",
       "       0.29767987, 0.65493693, 0.21859743, 0.90027904, 0.13013567,\n",
       "       0.81371562, 0.13610635, 0.84276502, 0.12700322, 0.87790232,\n",
       "       0.59808804, 0.12517601, 0.65490661, 0.11487155, 0.14412709,\n",
       "       0.25074609, 0.89267223, 0.11622218, 0.13790202, 0.34223771,\n",
       "       0.12796256, 0.19365149, 0.14018024, 0.80950131, 0.89791511,\n",
       "       0.87599955, 0.82599874, 0.33035454, 0.12104665, 0.33256695,\n",
       "       0.2871044 , 0.87904012, 0.16058594, 0.86243053, 0.59134008,\n",
       "       0.74587991, 0.1543381 , 0.39646483, 0.13353789, 0.12701466,\n",
       "       0.12101447, 0.13788495, 0.13013567, 0.83007385, 0.12700079,\n",
       "       0.10894619, 0.12701002, 0.85005234, 0.64931714, 0.16618664,\n",
       "       0.12104744, 0.21821031, 0.12101447, 0.5097587 , 0.14015932,\n",
       "       0.3449509 , 0.13788495, 0.91564324, 0.63329198, 0.13206702,\n",
       "       0.85715289, 0.15861211, 0.12499702, 0.14266702, 0.16811417,\n",
       "       0.52047246, 0.66229245, 0.65490661, 0.64138125, 0.71200672,\n",
       "       0.10600723, 0.12098591, 0.36277807, 0.13206749, 0.13013567,\n",
       "       0.33304406, 0.59320635, 0.13206749, 0.5058149 , 0.1208131 ,\n",
       "       0.12263198, 0.77904145, 0.1266512 , 0.33024405, 0.12028548,\n",
       "       0.11813558, 0.17546984, 0.12169028, 0.13346667, 0.65490661,\n",
       "       0.82135602, 0.3349679 , 0.67693366, 0.20916067, 0.42576549,\n",
       "       0.1391233 , 0.13798687, 0.12101686, 0.61905813, 0.90112575,\n",
       "       0.67394569, 0.23918442, 0.17328368, 0.12182407, 0.18522385,\n",
       "       0.12261977, 0.13490689, 0.16213663, 0.45541235, 0.9060203 ,\n",
       "       0.12509399, 0.8656554 , 0.34597795, 0.14469227, 0.17033775,\n",
       "       0.82149328, 0.32822924, 0.13206702, 0.64323718, 0.12182816,\n",
       "       0.25111353, 0.15333007, 0.09369676, 0.20950322, 0.35409118,\n",
       "       0.1750671 , 0.11811901, 0.14695545, 0.91556576, 0.33656009,\n",
       "       0.61838844, 0.16213663, 0.6246373 , 0.16542449, 0.85159855,\n",
       "       0.89604589, 0.16322206, 0.24472224, 0.16066254, 0.70032835,\n",
       "       0.15642285, 0.85674382, 0.12104585, 0.13788495, 0.57256735,\n",
       "       0.10418161, 0.87673302, 0.86920135, 0.13097708, 0.9191463 ,\n",
       "       0.15714899, 0.13129579, 0.53324137, 0.89563745, 0.17355165,\n",
       "       0.15319342, 0.90892039, 0.16307814, 0.13130214, 0.87656306,\n",
       "       0.90969631, 0.48855368, 0.17001886, 0.19866738, 0.13509335,\n",
       "       0.13788495, 0.14009086, 0.54135379, 0.59500647, 0.15905205,\n",
       "       0.83278804, 0.124298  , 0.12019101, 0.14605329, 0.18787717,\n",
       "       0.38579213, 0.87751493, 0.56456941, 0.128075  , 0.10317864,\n",
       "       0.91170132, 0.14230296, 0.8877404 , 0.1260745 , 0.12970092,\n",
       "       0.90754457, 0.12634745, 0.90892342, 0.35988753, 0.30441689,\n",
       "       0.18965844, 0.15014721, 0.26821068, 0.65489976, 0.64587182,\n",
       "       0.65490661, 0.90712204, 0.56935712, 0.13013567, 0.86010034,\n",
       "       0.10126334, 0.13013567, 0.41848035])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
